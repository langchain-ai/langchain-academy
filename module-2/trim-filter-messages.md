# Filtering and trimming messages

## Review

これまでに以下の項目について深く理解してきました:

* グラフ状態スキーマのカスタマイズ方法
* カスタム状態レデューサーの定義方法
* 複数のグラフ状態スキーマの使用方法

## Goals

これからはLangGraphでモデルを使用してこれらの概念を実践していきます!

今後のセッションでは、長期的な記憶を持つチャットボットを構築していきます。

チャットボットはメッセージを使用するため、まずグラフ状態でメッセージを扱う高度な方法について説明します。

## Messages as state

まず、いくつかのメッセージを定義してみましょう。

[コード例]

これらをチャット モデルに渡すことができることを思い出してください。

[コード例]

MessagesStateを使って、シンプルなグラフでチャットモデルを実行できます。

## Reducer 

メッセージを扱う際の実際的な課題は、長時間実行される会話を管理することです。

長時間実行される会話は、増え続けるメッセージのリストをモデルに渡すため、注意しないとトークンの使用率とレイテンシが高くなります。

これに対処するいくつかの方法があります。

まず、`RemoveMessage` と `add_messages` リデューサーを使用して見たテクニックを思い出してください。

[コード例]

## Filtering messages

グラフの状態を変更する必要がない場合や変更したくない場合は、チャット モデルに渡すメッセージをフィルタリングするだけで済みます。

たとえば、フィルタリングされたリスト `llm.invoke(messages[-1:])` をモデルに渡すだけです。

既存のメッセージ リストを取得し、上記の LLM 応答を追加して、フォローアップの質問を追加してみましょう。

状態にはすべてのメッセージがあります。

LangSmith トレースを見て、モデルの呼び出しで最後のメッセージのみが使用されていることを確認してください。

## Trim messages

もう1つのアプローチは、指定したトークン数に基づいて [メッセージをトリミング](https://python.langchain.com/v0.2/docs/how_to/trim_messages/#getting-the-last-max_tokens-tokens) する方法があります。

これにより、メッセージ履歴が指定されたトークンの数に制限されます。

フィルタリングがエージェント間のメッセージのサブセットを事後的に返すのに対し、トリミングはチャットモデルが応答に使用できるトークン数を制限します。

以下の`trim_messages`を参照してください。

LangSmith トレースを見て、モデルの呼び出しを確認してみましょう。
