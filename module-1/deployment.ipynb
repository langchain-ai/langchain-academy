{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e83a41b",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-1/deployment.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239303-lesson-8-deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20242c4-0010-4065-89f6-0e0b16c7da6e",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "## Review \n",
    "\n",
    "We built up to an agent with memory: \n",
    "\n",
    "* `act` - let the model call specific tools \n",
    "* `observe` - pass the tool output back to the model \n",
    "* `reason` - let the model reason about the tool output to decide what to do next (e.g., call another tool or just respond directly)\n",
    "* `persist state` - use an in memory checkpointer to support long-running conversations with interruptions\n",
    "\n",
    "## Goals\n",
    "\n",
    "Now, we'll cover how to actually deploy our agent locally to Studio and to `LangGraph Cloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f348498b-f277-4514-b163-fe5ed9afe6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph_sdk langchain_core"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4d0f4a7-82ee-4458-bd9a-e246ce2dc4ae",
   "metadata": {},
   "source": [
    "## Concepts\n",
    "\n",
    "There are a few central concepts to understand -\n",
    "\n",
    "`LangGraph` â€”\n",
    "- Python and JavaScript library \n",
    "- Allows creation of agent workflows \n",
    "\n",
    "`LangGraph API` â€”\n",
    "- Bundles the graph code \n",
    "- Provides a task queue for managing asynchronous operations\n",
    "- Offers persistence for maintaining state across interactions\n",
    "\n",
    "`LangGraph Cloud` --\n",
    "- Hosted service for the LangGraph API\n",
    "- Allows deployment of graphs from GitHub repositories\n",
    "- Also provides monitoring and tracing for deployed graphs\n",
    "- Accessible via a unique URL for each deployment\n",
    "\n",
    "`LangGraph Studio` --\n",
    "- Integrated Development Environment (IDE) for LangGraph applications\n",
    "- Uses the API as its back-end, allowing real-time testing and exploration of graphs\n",
    "- Can be run locally or with cloud-deployment\n",
    "\n",
    "`LangGraph SDK` --\n",
    "- Python library for programmatically interacting with LangGraph graphs\n",
    "- Provides a consistent interface for working with graphs, whether served locally or in the cloud\n",
    "- Allows creation of clients, access to assistants, thread management, and execution of runs\n",
    "\n",
    "## Testing Locally\n",
    "\n",
    "**âš ï¸ DISCLAIMER**\n",
    "\n",
    "Since the filming of these videos, we've updated Studio so that it can be run locally and opened in your browser. This is now the preferred way to run Studio (rather than using the Desktop App as shown in the video). See documentation [here](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server) on the local development server and [here](https://langchain-ai.github.io/langgraph/how-tos/local-studio/#run-the-development-server). To start the local development server, run the following command in your terminal in the `/studio` directory in this module:\n",
    "\n",
    "```\n",
    "langgraph dev\n",
    "```\n",
    "\n",
    "You should see the following output:\n",
    "```\n",
    "- ðŸš€ API: http://127.0.0.1:2024\n",
    "- ðŸŽ¨ Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "- ðŸ“š API Docs: http://127.0.0.1:2024/docs\n",
    "```\n",
    "\n",
    "Open your browser and navigate to the Studio UI: `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa75ebd4-91fe-42c5-8122-c81e72133477",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    raise Exception(\"Unfortunately LangGraph Studio is currently not supported on Google Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18b281d8-bd07-4721-922c-347838ceee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_sdk import get_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c96f353-5dc3-41c8-a3e4-6bf07ca455f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectError",
     "evalue": "All connection attempts failed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpx/_transports/default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpx/_transports/default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:256\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:236\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m connection.handle_async_request(\n\u001b[32m    237\u001b[39m         pool_request.request\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpcore/_async/connection.py:101\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_async_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpcore/_async/connection.py:78\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connect(request)\n\u001b[32m     80\u001b[39m     ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpcore/_async/connection.py:124\u001b[39m, in \u001b[36mAsyncHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_backend.connect_tcp(**kwargs)\n\u001b[32m    125\u001b[39m     trace.return_value = stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpcore/_backends/auto.py:31\u001b[39m, in \u001b[36mAutoBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._init_backend()\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.connect_tcp(\n\u001b[32m     32\u001b[39m     host,\n\u001b[32m     33\u001b[39m     port,\n\u001b[32m     34\u001b[39m     timeout=timeout,\n\u001b[32m     35\u001b[39m     local_address=local_address,\n\u001b[32m     36\u001b[39m     socket_options=socket_options,\n\u001b[32m     37\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpcore/_backends/anyio.py:113\u001b[39m, in \u001b[36mAnyIOBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m    108\u001b[39m exc_map = {\n\u001b[32m    109\u001b[39m     \u001b[38;5;167;01mTimeoutError\u001b[39;00m: ConnectTimeout,\n\u001b[32m    110\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m    111\u001b[39m     anyio.BrokenResourceError: ConnectError,\n\u001b[32m    112\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43manyio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfail_after\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpcore/_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: All connection attempts failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m client = get_client(url=URL)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Search all hosted graphs\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m assistants = \u001b[38;5;28;01mawait\u001b[39;00m client.assistants.search()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/langgraph_sdk/client.py:901\u001b[39m, in \u001b[36mAssistantsClient.search\u001b[39m\u001b[34m(self, metadata, graph_id, limit, offset, sort_by, sort_order, headers)\u001b[39m\n\u001b[32m    899\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sort_order:\n\u001b[32m    900\u001b[39m     payload[\u001b[33m\"\u001b[39m\u001b[33msort_order\u001b[39m\u001b[33m\"\u001b[39m] = sort_order\n\u001b[32m--> \u001b[39m\u001b[32m901\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.http.post(\n\u001b[32m    902\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/assistants/search\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    903\u001b[39m     json=payload,\n\u001b[32m    904\u001b[39m     headers=headers,\n\u001b[32m    905\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/langgraph_sdk/client.py:286\u001b[39m, in \u001b[36mHttpClient.post\u001b[39m\u001b[34m(self, path, json, headers, on_response)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m headers:\n\u001b[32m    285\u001b[39m     request_headers.update(headers)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m r = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.post(path, headers=request_headers, content=content)\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_response:\n\u001b[32m    288\u001b[39m     on_response(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpx/_client.py:1859\u001b[39m, in \u001b[36mAsyncClient.post\u001b[39m\u001b[34m(self, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m   1838\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1839\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1840\u001b[39m     url: URL | \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1852\u001b[39m     extensions: RequestExtensions | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1853\u001b[39m ) -> Response:\n\u001b[32m   1854\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1855\u001b[39m \u001b[33;03m    Send a `POST` request.\u001b[39;00m\n\u001b[32m   1856\u001b[39m \n\u001b[32m   1857\u001b[39m \u001b[33;03m    **Parameters**: See `httpx.request`.\u001b[39;00m\n\u001b[32m   1858\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1859\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(\n\u001b[32m   1860\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1861\u001b[39m         url,\n\u001b[32m   1862\u001b[39m         content=content,\n\u001b[32m   1863\u001b[39m         data=data,\n\u001b[32m   1864\u001b[39m         files=files,\n\u001b[32m   1865\u001b[39m         json=json,\n\u001b[32m   1866\u001b[39m         params=params,\n\u001b[32m   1867\u001b[39m         headers=headers,\n\u001b[32m   1868\u001b[39m         cookies=cookies,\n\u001b[32m   1869\u001b[39m         auth=auth,\n\u001b[32m   1870\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1871\u001b[39m         timeout=timeout,\n\u001b[32m   1872\u001b[39m         extensions=extensions,\n\u001b[32m   1873\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpx/_client.py:1540\u001b[39m, in \u001b[36mAsyncClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m   1525\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m   1527\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m   1528\u001b[39m     method=method,\n\u001b[32m   1529\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1538\u001b[39m     extensions=extensions,\n\u001b[32m   1539\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1540\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.send(request, auth=auth, follow_redirects=follow_redirects)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpx/_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m   1625\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m   1630\u001b[39m     request,\n\u001b[32m   1631\u001b[39m     auth=auth,\n\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1633\u001b[39m     history=[],\n\u001b[32m   1634\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpx/_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m   1654\u001b[39m request = \u001b[38;5;28;01mawait\u001b[39;00m auth_flow.\u001b[34m__anext__\u001b[39m()\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m   1658\u001b[39m         request,\n\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1660\u001b[39m         history=history,\n\u001b[32m   1661\u001b[39m     )\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1663\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpx/_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m   1691\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1696\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpx/_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1725\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1726\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1727\u001b[39m     )\n\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n\u001b[32m   1733\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpx/_transports/default.py:393\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhttpcore\u001b[39;00m\n\u001b[32m    381\u001b[39m req = httpcore.Request(\n\u001b[32m    382\u001b[39m     method=request.method,\n\u001b[32m    383\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     extensions=request.extensions,\n\u001b[32m    392\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mawait\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_async_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    156\u001b[39m     value = typ()\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpx/_transports/default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: All connection attempts failed"
     ]
    }
   ],
   "source": [
    "# This is the URL of the local development server\n",
    "URL = \"http://127.0.0.1:2024\"\n",
    "client = get_client(url=URL)\n",
    "\n",
    "# Search all hosted graphs\n",
    "assistants = await client.assistants.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a1352fa-68ad-4963-890e-c95d93570917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca',\n",
       " 'graph_id': 'agent',\n",
       " 'config': {},\n",
       " 'metadata': {'created_by': 'system'},\n",
       " 'name': 'agent',\n",
       " 'created_at': '2025-03-04T22:57:28.424565+00:00',\n",
       " 'updated_at': '2025-03-04T22:57:28.424565+00:00',\n",
       " 'version': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistants[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba9c28a0-d712-496c-b191-7d620589ba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a thread for tracking the state of our run\n",
    "thread = await client.threads.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7e4177-3644-43fa-a2f1-08f73292d1a6",
   "metadata": {},
   "source": [
    "Now, we can run our agent [with `client.runs.stream`](https://langchain-ai.github.io/langgraph/concepts/low_level/#stream-and-astream) with:\n",
    "\n",
    "* The `thread_id`\n",
    "* The `graph_id`\n",
    "* The `input` \n",
    "* The `stream_mode`\n",
    "\n",
    "We'll discuss streaming in depth in a future module. \n",
    "\n",
    "For now, just recognize that we are [streaming](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_values/) the full value of the state after each step of the graph with `stream_mode=\"values\"`.\n",
    " \n",
    "The state is captured in the `chunk.data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f65a4480-66b3-48bf-9158-191a7b8c1c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Multiply 3 by 2.', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'cdbd7bd8-c476-4ad4-8ab7-4ad9e3654267', 'example': False}\n",
      "{'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'function': {'arguments': '{\"a\":3,\"b\":2}', 'name': 'multiply'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, 'type': 'ai', 'name': None, 'id': 'run-06c7243c-426d-4c81-a113-f1335dda5fb2', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 3, 'b': 2}, 'id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': None}\n",
      "{'content': '6', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '988cb170-f6e6-43c1-82fd-309f519abe6d', 'tool_call_id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'artifact': None, 'status': 'success'}\n",
      "{'content': 'The result of multiplying 3 by 2 is 6.', 'additional_kwargs': {}, 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, 'type': 'ai', 'name': None, 'id': 'run-7bda0aa0-6895-4250-9625-18419c5dc171', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Input\n",
    "input = {\"messages\": [HumanMessage(content=\"Multiply 3 by 2.\")]}\n",
    "\n",
    "# Stream\n",
    "async for chunk in client.runs.stream(\n",
    "        thread['thread_id'],\n",
    "        \"agent\",\n",
    "        input=input,\n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "    if chunk.data and chunk.event != \"metadata\":\n",
    "        print(chunk.data['messages'][-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfa8b850-750c-4054-95e4-1c457a12ec8a",
   "metadata": {},
   "source": [
    "## Testing with Cloud\n",
    "\n",
    "We can deploy to Cloud via LangSmith, as outlined [here](https://langchain-ai.github.io/langgraph/cloud/quick_start/#deploy-from-github-with-langgraph-cloud). \n",
    "\n",
    "### Create a New Repository on GitHub\n",
    "\n",
    "* Go to your GitHub account\n",
    "* Click on the \"+\" icon in the upper-right corner and select `\"New repository\"`\n",
    "* Name your repository (e.g., `langchain-academy`)\n",
    "\n",
    "### Add Your GitHub Repository as a Remote\n",
    "\n",
    "* Go back to your terminal where you cloned `langchain-academy` at the start of this course\n",
    "* Add your newly created GitHub repository as a remote\n",
    "\n",
    "```\n",
    "git remote add origin https://github.com/your-username/your-repo-name.git\n",
    "```\n",
    "* Push to it\n",
    "```\n",
    "git push -u origin main\n",
    "```\n",
    "\n",
    "### Connect LangSmith to your GitHub Repository\n",
    "\n",
    "* Go to [LangSmith](hhttps://smith.langchain.com/)\n",
    "* Click on `deployments` tab on the left LangSmith panel\n",
    "* Add `+ New Deployment`\n",
    "* Then, select the Github repository (e.g., `langchain-academy`) that you just created for the course\n",
    "* Point the `LangGraph API config file` at one of the `studio` directories\n",
    "* For example, for module-1 use: `module-1/studio/langgraph.json`\n",
    "* Set your API keys (e.g., you can just copy from your `module-1/studio/.env` file)\n",
    "\n",
    "![Screenshot 2024-09-03 at 11.35.12 AM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbad4fd61c93d48e5d0f47_deployment2.png)\n",
    "\n",
    "### Work with your deployment\n",
    "\n",
    "We can then interact with our deployment a few different ways:\n",
    "\n",
    "* With the [SDK](https://langchain-ai.github.io/langgraph/cloud/quick_start/#use-with-the-sdk), as before.\n",
    "* With [LangGraph Studio](https://langchain-ai.github.io/langgraph/cloud/quick_start/#interact-with-your-deployment-via-langgraph-studio).\n",
    "\n",
    "![Screenshot 2024-08-23 at 10.59.36 AM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbad4fa159a09a51d601de_deployment3.png)\n",
    "\n",
    "To use the SDK here in the notebook, simply ensure that `LANGSMITH_API_KEY` is set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "646ed351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"LANGSMITH_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dda16c-c87f-4c03-b910-d647e83400b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with the URL of your deployed graph\n",
    "URL = \"https://langchain-academy-8011c561878d50b1883f7ed11b32d720.default.us.langgraph.app\"\n",
    "client = get_client(url=URL)\n",
    "\n",
    "# Search all hosted graphs\n",
    "assistants = await client.assistants.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aefa37c0-92fe-4e80-9d5a-80a77b1e3dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the agent\n",
    "agent = assistants[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b810376e-f20f-443a-b1ca-d6793f358f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca',\n",
       " 'graph_id': 'agent',\n",
       " 'created_at': '2024-08-23T17:58:02.722920+00:00',\n",
       " 'updated_at': '2024-08-23T17:58:02.722920+00:00',\n",
       " 'config': {},\n",
       " 'metadata': {'created_by': 'system'}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32d65d84-1bcf-4af4-a7c9-55e73d6c1947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Multiply 3 by 2.', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '8ea04559-f7d4-4c82-89d9-c60fb0502f21', 'example': False}\n",
      "{'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_EQoolxFaaSVU8HrTnCmffLk7', 'function': {'arguments': '{\"a\":3,\"b\":2}', 'name': 'multiply'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_3aa7262c27'}, 'type': 'ai', 'name': None, 'id': 'run-b0ea5ddd-e9ba-4242-bb8c-80eb52466c76', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 3, 'b': 2}, 'id': 'call_EQoolxFaaSVU8HrTnCmffLk7', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': None}\n",
      "{'content': '6', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '1bf558e7-79ef-4f21-bb66-acafbd04677a', 'tool_call_id': 'call_EQoolxFaaSVU8HrTnCmffLk7', 'artifact': None, 'status': 'success'}\n",
      "{'content': '3 multiplied by 2 equals 6.', 'additional_kwargs': {}, 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_3aa7262c27'}, 'type': 'ai', 'name': None, 'id': 'run-ecc4b6ad-af15-4a85-a76c-de2ed0ed8ed9', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# We create a thread for tracking the state of our run\n",
    "thread = await client.threads.create()\n",
    "\n",
    "# Input\n",
    "input = {\"messages\": [HumanMessage(content=\"Multiply 3 by 2.\")]}\n",
    "\n",
    "# Stream\n",
    "async for chunk in client.runs.stream(\n",
    "        thread['thread_id'],\n",
    "        \"agent\",\n",
    "        input=input,\n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "    if chunk.data and chunk.event != \"metadata\":\n",
    "        print(chunk.data['messages'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445cb34d-c3b8-4446-a7e3-5fe938abf99b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
